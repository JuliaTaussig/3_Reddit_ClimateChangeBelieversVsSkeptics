{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries:\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figuring Out How to Collect Data (See End of Notebook for Collection of Data that Was Used for Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting URL's to gather info from:\n",
    "\n",
    "#The url with discussion about climate change (probably mostly believers in climate change):\n",
    "url_1 = 'https://www.reddit.com/r/climatechange.json'\n",
    "\n",
    "#The url with skeptic discussion about climate change (probably mostly climate change skeptics):\n",
    "url_2 = 'https://www.reddit.com/r/climateskeptics.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a header so reddit.com sees a header instead of a computer looking for infomation on the site:\n",
    "#this helps to get a return res.status_code of 200 instead of 429 error\n",
    "headers = {'User-agent' : 'Julia bot 0.1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gathering data from the urls (the data comes in as json objects):\n",
    "res_1 = requests.get(url_1, headers = headers)\n",
    "res_2 = requests.get(url_2, headers = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "#Checking have 200 rather than 429 error (making sure gathering requests without client error or other errors)\n",
    "print(res_1.status_code)\n",
    "print(res_2.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n",
      "<class 'requests.models.Response'>\n"
     ]
    }
   ],
   "source": [
    "#Checking data types of res_1 and res_2\n",
    "print(type(res_1))\n",
    "print(type(res_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting response json objects to Python dictionaries\n",
    "data_1 = res_1.json()\n",
    "data_2 = res_2.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#Checking data types of data_1 and data_2 (should be dictionaries)\n",
    "print(type(data_1))\n",
    "print(type(data_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kind', 'data'])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking keys of data_1:\n",
    "data_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kind', 'data'])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking keys of data_2:\n",
    "data_2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Listing'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking what inside 'kind' key in data_1 (no keys inside)\n",
    "data_1['kind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Listing'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking what inside 'kind' key in data_2 (no keys inside)\n",
    "data_2['kind']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there is only information stating that the information in the kind key is the statement that the kind of information in data_1 and data_2 is 'Listing.'  We'll look inside the 'data' key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['modhash', 'dist', 'children', 'after', 'before'])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking what inside 'data' key in data_1\n",
    "data_1['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['modhash', 'dist', 'children', 'after', 'before'])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking what inside 'data' key in data_2\n",
    "data_2['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at the number of posts collected in data_1:\n",
    "len(data_1['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at the number of posts collected in data_2:\n",
    "len(data_2['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at 1st item (index 0) of the posts:\n",
    "#data_1['data']['children'][0]['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subreddit rules'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking to see what info in the following location:\n",
    "data_1['data']['children'][0]['data']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm afraid climate change is going to kill me! Help!\""
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at 2nd item (index 1) of the posts:\n",
    "data_1['data']['children'][1]['data']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'More bang for the climate buck: Study identifies hotspots for adaptation funding'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at 3rd item (index 2) of the posts:\n",
    "data_1['data']['children'][2]['data']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alpine glaciers: Another decade of loss'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at 4th item (index 3) of the posts:\n",
    "data_1['data']['children'][3]['data']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sources of Variation in Climate Sensitivity Estimates (PDF)'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at 5th item (index 4) of the posts:\n",
    "data_1['data']['children'][4]['data']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Has eco-anxiety influenced your parenting/parenthood decisions?'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at 6th item (index 5) of the posts:\n",
    "data_1['data']['children'][5]['data']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looks like the first two posts are about the subreddit rules and sources to read, respectively,\n",
    "#so the third item (index 2) and on will be collected for this model:\n",
    "#data_1['data']['children'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The url with discussion about climate change (assuming mostly believers in climate change):\n",
    "url_1 = 'https://www.reddit.com/r/climate.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting header to look like Julia bot 0.1 instead of a program when requeting data from the API:\n",
    "headers = {'User-agent' : 'Julia bot 0.1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requesting data from url_1 using the header created earlier:\n",
    "res_1 = requests.get(url_1, headers = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "#Checking the status of res_1 (if in 200's good, and if in 400's client-side error)\n",
    "print(res_1.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Savind data in json format to data_1:\n",
    "data_1 = res_1.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking length - seeing how many posts pulling in one go:\n",
    "len(data_1['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting data_1\n",
    "#data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries for use during while loops when will request a lot of data:\n",
    "#Noe: decided not to use the clear_output library later on\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "125\n",
      "150\n",
      "175\n",
      "200\n",
      "225\n",
      "250\n",
      "275\n",
      "300\n",
      "325\n",
      "350\n",
      "375\n",
      "400\n",
      "425\n",
      "450\n",
      "475\n",
      "500\n",
      "525\n",
      "550\n",
      "575\n",
      "600\n",
      "625\n",
      "650\n",
      "675\n",
      "700\n",
      "725\n",
      "750\n",
      "775\n",
      "800\n",
      "825\n",
      "850\n",
      "875\n",
      "900\n",
      "925\n",
      "934\n"
     ]
    }
   ],
   "source": [
    "#Collecting posts on the r/climatechange/new page (note: decided to use r/climate subreddit instead of \n",
    "#r/climatechange subreddit)\n",
    "\n",
    "#Initializing counter:\n",
    "counter = 0\n",
    "#Creating a header which will prevent request-related error: \n",
    "#(instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\")\n",
    "headers = {'User-agent': 'Julia bot 0.3'}\n",
    "#Initializing after variable as True intil it will be replaced with after values in posts:\n",
    "after = True\n",
    "#Initializing posts_climate_change list and params dictionary:\n",
    "posts_climate_change = []\n",
    "params = {}\n",
    "#Limiting the loop so 100*25 = 2500 or less posts collected: \n",
    "while counter < 100:\n",
    "    #Making sure that the loop breaks once the final post is collected (when after field is empty):\n",
    "    if after == None:\n",
    "        break\n",
    "    #Collecting posts that will be placed in the posts_climate_change list:\n",
    "    else:\n",
    "        #Filling the current post's after value in the params dictionary:\n",
    "        params = {'after': after}\n",
    "    #Requesting data from the Reddit API at https://www.reddit.com/r/climatechange/new.json\n",
    "    #Using the params dictionary to ensure collecting posts sequentially \n",
    "    #Using headers so instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\":\n",
    "    res = requests.get('https://www.reddit.com/r/climatechange/new.json', params, headers=headers)\n",
    "    #Using the data in the json format:\n",
    "    the_json = res.json()\n",
    "    #Getting each full post (with all information related to each post):\n",
    "    posts_climate_change.extend(the_json['data'].get('children'))\n",
    "    #Getting the after value for this post so it can be used in params to collect the next post\n",
    "    after = the_json['data']['after']\n",
    "    #Adding to counter each loop of the while loop\n",
    "    counter += 1\n",
    "    #Time delay so appear as a human clicking through the subreddit instead of a computer\n",
    "    #(also trying to respect Reddit - don't want to overload their servers)\n",
    "    time.sleep(1)\n",
    "    #Printing the length of posts_climate_change to see running total length of \n",
    "    #posts_climate_change while this cell runs\n",
    "    print(len(posts_climate_change))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "125\n",
      "150\n",
      "175\n",
      "200\n",
      "225\n",
      "250\n",
      "275\n",
      "300\n",
      "325\n",
      "350\n",
      "375\n",
      "400\n",
      "425\n",
      "450\n",
      "475\n",
      "500\n",
      "525\n",
      "550\n",
      "575\n",
      "600\n",
      "625\n",
      "650\n",
      "675\n",
      "700\n",
      "725\n",
      "750\n",
      "775\n",
      "800\n",
      "825\n",
      "850\n",
      "875\n",
      "900\n",
      "925\n",
      "950\n",
      "975\n",
      "976\n"
     ]
    }
   ],
   "source": [
    "#Collecting posts on the r/climate/new page\n",
    "\n",
    "#Initializing counter:\n",
    "counter = 0\n",
    "#Creating a header which will prevent request-related error: \n",
    "#(instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\")\n",
    "headers = {'User-agent': 'Julia bot 0.3'}\n",
    "#Initializing after variable as True intil it will be replaced with after values in posts:\n",
    "after = True\n",
    "#Initializing posts_climate list and params dictionary:\n",
    "posts_climate = []\n",
    "params = {}\n",
    "#Limiting the loop so 100*25 = 2500 or less posts collected: \n",
    "while counter < 100:\n",
    "    #Making sure that the loop breaks once the final post is collected (when after field is empty):\n",
    "    if after == None:\n",
    "        break\n",
    "    #Collecting posts that will be placed in the posts_climate list:\n",
    "    else:\n",
    "        #Filling the current post's after value in the params dictionary:\n",
    "        params = {'after': after}\n",
    "    #Requesting data from the Reddit API at https://www.reddit.com/r/climate/new.json\n",
    "    #Using the params dictionary to ensure collecting posts sequentially \n",
    "    #Using headers so instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\":\n",
    "    res = requests.get('https://www.reddit.com/r/climate/new.json', params, headers=headers)\n",
    "    #Using the data in the json format:\n",
    "    the_json = res.json()\n",
    "    #Getting each full post (with all information related to each post):\n",
    "    posts_climate.extend(the_json['data'].get('children'))\n",
    "    #Getting the after value for this post so it can be used in params to collect the next post\n",
    "    after = the_json['data']['after']\n",
    "    #Adding to counter each loop of the while loop\n",
    "    counter += 1\n",
    "    #Time delay so appear as a human clicking through the subreddit instead of a computer\n",
    "    #(also trying to respect Reddit - don't want to overload their servers)\n",
    "    time.sleep(1)\n",
    "    #Printing the length of posts_climate to see running total length of \n",
    "    #posts_climate while this cell runs\n",
    "    print(len(posts_climate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "976"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding how many posts_climate collected\n",
    "len(posts_climate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I'm wondering if 976 posts is enough and why the maximum of 1000 posts were not collected as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which 2020 Democrats Are Strongest on Climate? Here’s the List.'"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspecting title of first post in posts_climate list:\n",
    "posts_climate[0]['data']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating list of titles of posts in posts_climate:\n",
    "post_titles_list_climate = []\n",
    "for i in range(len(posts_climate)):\n",
    "    post_title_climate = posts_climate[i]['data']['title']\n",
    "    post_titles_list_climate.append(post_title_climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "976"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking have same number of titles in post_titles_list_climate as number of posts in posts_climate:\n",
    "len(post_titles_list_climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which 2020 Democrats Are Strongest on Climate?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Climate Change and the Death of the Small Farm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The GOP’s answer to catastrophic climate chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ocasio-Cortez’s righteous — and accurate — ang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Polar bear climate change</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Which 2020 Democrats Are Strongest on Climate?...\n",
       "1     Climate Change and the Death of the Small Farm\n",
       "2  The GOP’s answer to catastrophic climate chang...\n",
       "3  Ocasio-Cortez’s righteous — and accurate — ang...\n",
       "4                          Polar bear climate change"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making dataframe with post titles from r/climate/new page:\n",
    "df_post_titles_climate = pd.DataFrame(post_titles_list_climate)\n",
    "#Inspecting the head of the df:\n",
    "df_post_titles_climate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(976, 1)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspecting the dimensions of df_post_titles_climate:\n",
    "df_post_titles_climate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "125\n",
      "150\n",
      "175\n",
      "200\n",
      "225\n",
      "250\n",
      "275\n",
      "300\n",
      "325\n",
      "350\n",
      "375\n",
      "400\n",
      "425\n",
      "450\n",
      "475\n",
      "500\n",
      "525\n",
      "550\n",
      "575\n",
      "600\n",
      "625\n",
      "650\n",
      "675\n",
      "700\n",
      "725\n",
      "750\n",
      "775\n",
      "800\n",
      "825\n",
      "850\n",
      "875\n",
      "900\n",
      "925\n",
      "950\n",
      "975\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "#Collecting posts on the r/climateskeptics/new page\n",
    "\n",
    "#Initializing counter:\n",
    "counter = 0\n",
    "#Creating a header which will prevent request-related error: \n",
    "#(instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.4\")\n",
    "headers = {'User-agent': 'Julia bot 0.4'}\n",
    "#Initializing after variable as True intil it will be replaced with after values in posts:\n",
    "after = True\n",
    "#Initializing posts_skeptics list and params dictionary:\n",
    "posts_skeptics = []\n",
    "params = {}\n",
    "#Limiting the loop so 100*25 = 2500 or less posts collected:\n",
    "while counter < 100:\n",
    "    #Making sure that the loop breaks once the final post is collected (when after field is empty):\n",
    "    if after == None:\n",
    "        break\n",
    "    #Collecting posts that will be placed in the posts_skeptics list:\n",
    "    else:\n",
    "        #Filling the current post's after value in the params dictionary:\n",
    "        params = {'after': after}\n",
    "    #Requesting data from the Reddit API at https://www.reddit.com/r/climateskeptics/new.json\n",
    "    #Using the params dictionary to ensure collecting posts sequentially \n",
    "    #Using headers so instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.4\":\n",
    "    res = requests.get('https://www.reddit.com/r/climateskeptics/new.json', params, headers=headers)\n",
    "    #Using the data in the json format:\n",
    "    the_json = res.json()\n",
    "    #Getting each full post (with all information related to each post):\n",
    "    posts_skeptics.extend(the_json['data'].get('children'))\n",
    "    #Getting the after value for this post so it can be used in params to collect the next post\n",
    "    after = the_json['data']['after']\n",
    "    #Adding to counter each loop of the while loop\n",
    "    counter += 1\n",
    "    #Time delay so appear as a human clicking through the subreddit instead of a computer\n",
    "    #(also trying to respect Reddit - don't want to overload their servers)\n",
    "    time.sleep(1)\n",
    "    #Printing the length of posts_skeptics to see running total length of \n",
    "    #posts_skeptics while this cell runs\n",
    "    print(len(posts_skeptics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "125\n",
      "150\n",
      "175\n",
      "200\n",
      "225\n",
      "250\n",
      "275\n",
      "300\n",
      "325\n",
      "350\n",
      "375\n",
      "400\n",
      "425\n",
      "450\n",
      "475\n",
      "500\n",
      "525\n",
      "550\n",
      "575\n",
      "600\n",
      "625\n",
      "650\n",
      "675\n",
      "700\n",
      "725\n",
      "750\n",
      "775\n",
      "800\n",
      "825\n",
      "850\n",
      "875\n",
      "900\n",
      "925\n",
      "950\n",
      "975\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "#Collecting posts on the r/climateskeptics/new page\n",
    "#Note: using len(posts_skeptics) to limit while loop instead of counter here\n",
    "\n",
    "#Creating a header which will prevent request-related error: \n",
    "#(instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.4\")\n",
    "headers = {'User-agent': 'Julia bot 0.4'}\n",
    "#Initializing after variable as True intil it will be replaced with after values in posts:\n",
    "after = True\n",
    "#Initializing posts_skeptics list and params dictionary:\n",
    "posts_skeptics = []\n",
    "params = {}\n",
    "#Limiting the loop so 2500 or less posts collected:\n",
    "while len(posts_skeptics) < 2500:\n",
    "    #Making sure that the loop breaks once the final post is collected (when after field is empty):\n",
    "    if after == None:\n",
    "        break\n",
    "    #Collecting posts that will be placed in the posts_skeptics list:\n",
    "    else:\n",
    "        #Filling the current post's after value in the params dictionary:\n",
    "        params = {'after': after}\n",
    "    #Requesting data from the Reddit API at https://www.reddit.com/r/climateskeptics/new.json\n",
    "    #Using the params dictionary to ensure collecting posts sequentially \n",
    "    #Using headers so instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.4\":\n",
    "    res = requests.get('https://www.reddit.com/r/climateskeptics/new.json', params, headers=headers)\n",
    "    #Using the data in the json format:\n",
    "    the_json = res.json()\n",
    "    #Getting each full post (with all information related to each post):\n",
    "    posts_skeptics.extend(the_json['data'].get('children'))\n",
    "    #Getting the after value for this post so it can be used in params to collect the next post\n",
    "    after = the_json['data']['after']\n",
    "    #Time delay so appear as a human clicking through the subreddit instead of a computer\n",
    "    #(also trying to respect Reddit - don't want to overload their servers)\n",
    "    time.sleep(1)\n",
    "    #Printing the length of posts_skeptics to see running total length of \n",
    "    #posts_skeptics while this cell runs\n",
    "    print(len(posts_skeptics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding number of posts collected from r/climateskeptics/new page:\n",
    "len(posts_skeptics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IEA: solar’s exponential growth could make it less competitive, not more. New VALCOE metric exposes inherent demand-matching issue'"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspecting the first post's title in posts_skeptics:\n",
    "posts_skeptics[0]['data']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating list of titles of posts in posts_skeptics:\n",
    "post_titles_list_skeptics = []\n",
    "for i in range(len(posts_skeptics)):\n",
    "    post_title_skeptics = posts_skeptics[i]['data']['title']\n",
    "    post_titles_list_skeptics.append(post_title_skeptics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking have same number of titles in post_titles_list_skeptics as posts in posts_skeptics\n",
    "len(post_titles_list_skeptics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IEA: solar’s exponential growth could make it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Faith vs Belief 2 Climate Change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The climate alarmists are keeping poor people ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Greenland Has Gained 510 Billion Tons of Ice O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Video: The Great Flood Myth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  IEA: solar’s exponential growth could make it ...\n",
       "1                   Faith vs Belief 2 Climate Change\n",
       "2  The climate alarmists are keeping poor people ...\n",
       "3  Greenland Has Gained 510 Billion Tons of Ice O...\n",
       "4                    New Video: The Great Flood Myth"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making dataframe with post titles from r/climateskeptics/new page:\n",
    "df_post_titles_skeptics = pd.DataFrame(post_titles_list_skeptics)\n",
    "#Inspecting the head of the df:\n",
    "df_post_titles_skeptics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking dimensions of df_post_titles_skeptics:\n",
    "df_post_titles_skeptics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving dataframe of r/climate/new post titles to a csv in the data file in this repo:\n",
    "df_post_titles_climate.to_csv('../data/climate_post_titles_20190328_0915.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving dataframe of r/climateskeptics/new post titles to a csv in the data file in this repo:\n",
    "df_post_titles_skeptics.to_csv('../data/climateskeptics_post_titles_20190328_0915.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting full posts in posts_climate_list and \n",
    "#post titles in post_titles_climate_list\n",
    "\n",
    "#Instantiating the lists:\n",
    "posts_climate_list = []\n",
    "post_titles_climate_list = []\n",
    "#Collecting posts and post titles in posts_climate and placing in lists:\n",
    "for i in range(len(posts_climate)):\n",
    "    posts_climate_list.append(posts_climate[i])\n",
    "    post_titles_climate_list.append(posts_climate[i]['data']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "976"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding number of posts in posts_climate_list:\n",
    "len(posts_climate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "976"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding number of post titles in post_titles_climate_list (should match the number of posts)\n",
    "len(post_titles_climate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>full_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which 2020 Democrats Are Strongest on Climate?...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Climate Change and the Death of the Small Farm</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The GOP’s answer to catastrophic climate chang...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ocasio-Cortez’s righteous — and accurate — ang...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Polar bear climate change</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Which 2020 Democrats Are Strongest on Climate?...   \n",
       "1     Climate Change and the Death of the Small Farm   \n",
       "2  The GOP’s answer to catastrophic climate chang...   \n",
       "3  Ocasio-Cortez’s righteous — and accurate — ang...   \n",
       "4                          Polar bear climate change   \n",
       "\n",
       "                                           full_post  \n",
       "0  {'kind': 't3', 'data': {'approved_at_utc': Non...  \n",
       "1  {'kind': 't3', 'data': {'approved_at_utc': Non...  \n",
       "2  {'kind': 't3', 'data': {'approved_at_utc': Non...  \n",
       "3  {'kind': 't3', 'data': {'approved_at_utc': Non...  \n",
       "4  {'kind': 't3', 'data': {'approved_at_utc': Non...  "
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiating dataframe to place posts and post titles into:\n",
    "df_posts_climate = pd.DataFrame()\n",
    "#Adding post_titles_climate_list to 'title' column of df:\n",
    "df_posts_climate['title'] = post_titles_climate_list\n",
    "#Adding posts_climate_list to 'full_post' column of df:\n",
    "df_posts_climate['full_post'] = posts_climate_list\n",
    "#Inspecting the head of the df:\n",
    "df_posts_climate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(976, 2)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspecting dimensions of the df:\n",
    "df_posts_climate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving dataframe of r/climate/new posts and post titles to a csv in the data file in this repo:\n",
    "df_posts_climate.to_csv('../data/climate_posts_with_titles_20190328_0915.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting full posts in posts_skeptics_list and \n",
    "#post titles in post_titles_skeptics_list\n",
    "\n",
    "#Instantiating the lists:\n",
    "posts_skeptics_list = []\n",
    "post_titles_skeptics_list = []\n",
    "#Collecting posts and post titles in posts_skeptics and placing in lists:\n",
    "for i in range(len(posts_skeptics)):\n",
    "    posts_skeptics_list.append(posts_skeptics[i])\n",
    "    post_titles_skeptics_list.append(posts_skeptics[i]['data']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding number of posts in posts_skeptics_list:\n",
    "len(posts_skeptics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding number of post titles in post_titles_skeptics_list (should match the number of posts)\n",
    "len(post_titles_skeptics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>full_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Professor Valentina Zharkova: The Solar Magnet...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IEA: solar’s exponential growth could make it ...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Faith vs Belief 2 Climate Change</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The climate alarmists are keeping poor people ...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Greenland Has Gained 510 Billion Tons of Ice O...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Professor Valentina Zharkova: The Solar Magnet...   \n",
       "1  IEA: solar’s exponential growth could make it ...   \n",
       "2                   Faith vs Belief 2 Climate Change   \n",
       "3  The climate alarmists are keeping poor people ...   \n",
       "4  Greenland Has Gained 510 Billion Tons of Ice O...   \n",
       "\n",
       "                                           full_post  \n",
       "0  {'kind': 't3', 'data': {'approved_at_utc': Non...  \n",
       "1  {'kind': 't3', 'data': {'approved_at_utc': Non...  \n",
       "2  {'kind': 't3', 'data': {'approved_at_utc': Non...  \n",
       "3  {'kind': 't3', 'data': {'approved_at_utc': Non...  \n",
       "4  {'kind': 't3', 'data': {'approved_at_utc': Non...  "
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiating dataframe to place posts and post titles into:\n",
    "df_posts_skeptics = pd.DataFrame()\n",
    "#Adding post_titles_skeptics_list to 'title' column of df:\n",
    "df_posts_skeptics['title'] = post_titles_skeptics_list\n",
    "#Adding posts_climate_list to 'full_post' column of df:\n",
    "df_posts_skeptics['full_post'] = posts_skeptics_list\n",
    "#Inspecting the head of the df:\n",
    "df_posts_skeptics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving dataframe of r/climateskeptics/new posts and post titles to a csv in the data file in this repo:\n",
    "df_posts_skeptics.to_csv('../data/skeptics_posts_with_titles_20190328_0915.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attaining Post Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.reddit.com/r/climate/comments/b6iojd/which_2020_democrats_are_strongest_on_climate/'"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a sample url for testing:\n",
    "'https://www.reddit.com' + posts_climate[0]['data']['permalink']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requesting .json data from the url:\n",
    "res = requests.get('https://www.reddit.com' + posts_climate[0]['data']['permalink']+'.json', headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensuring the data is in json format and  \n",
    "the_json = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We know Bernie is actually the best, so let's stop pretending like there's a lot of choice and unify behind one strong candidate that already had very good chances last election.\""
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspecting comments within the data (looks like there are layers of comments):\n",
    "the_json[1]['data']['children'][0]['data']['body']\n",
    "#len(the_json[1]['data']['children'][0]['data'])\n",
    "#len(the_json[1]['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"We know Bernie is actually the best, so let's stop pretending like there's a lot of choice and unify behind one strong candidate that already had very good chances last election.\",\n",
       " 'My \"fantasy football\" list is Bernie + Andrew Yang as VP= Win  They both coincide on income and environmental factors.  Andrew Yang is considering situations that are twenty years from now.  These are two people that are not just considering what transpires in their terms but what further effects would be.  Edit: too many effects!\\n\\n']"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating list of first-level comments on the page (collecting nested replies to these comments requires\n",
    "#some more work (struggling to figure out how to do this efficiently)\n",
    "list_i = []\n",
    "for i in range(len(the_json[1]['data']['children'])):    \n",
    "    #print(i)\n",
    "    list_i.append(the_json[1]['data']['children'][i]['data']['body'])\n",
    "\n",
    "len(list_i)\n",
    "#Looking at the list to see which comments were collected from the sample url:\n",
    "list_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing tried to find post comments but had some issues, so commented this out:\n",
    "\n",
    "# headers = {'User-agent': 'Julia bot 0.4'}\n",
    "# climate_comments_all = []\n",
    "\n",
    "# for i in range(len(post_titles_climate_list)):\n",
    "#     res_comments = requests.get('https://www.reddit.com' + \n",
    "#                                 posts_climate[i]['data']['permalink'] +'.json', headers=headers)\n",
    "#     the_json = res_comments.json()\n",
    "#     comments_each_post = []\n",
    "#     for j in range(len(the_json[1]['data']['children'])):        \n",
    "#         if j == []:\n",
    "#             comments_each_post.append(['No comments'])\n",
    "#         else:\n",
    "#             comments_each_post.append(the_json[1]['data']['children'][j]['data']['body'])\n",
    "        \n",
    "#         time.sleep(1)\n",
    "    \n",
    "#     climate_comments_all.append(comments_each_post)\n",
    "        \n",
    "#     clear_output()\n",
    "#     print(len(climate_comments_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "125\n",
      "150\n",
      "175\n",
      "200\n",
      "225\n",
      "250\n",
      "275\n",
      "300\n",
      "325\n",
      "350\n",
      "375\n",
      "400\n",
      "425\n",
      "450\n",
      "475\n",
      "500\n",
      "525\n",
      "550\n",
      "575\n",
      "600\n",
      "625\n",
      "650\n",
      "675\n",
      "700\n",
      "725\n",
      "750\n",
      "775\n",
      "800\n",
      "825\n",
      "850\n",
      "875\n",
      "900\n",
      "925\n",
      "950\n",
      "975\n",
      "976\n"
     ]
    }
   ],
   "source": [
    "#Collecting posts on the r/climate/new page\n",
    "\n",
    "#Creating a header which will prevent request-related error: \n",
    "#(instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\")\n",
    "headers = {'User-agent': 'Julia bot 0.3'}\n",
    "#Initializing after variable as True intil it will be replaced with after values in posts:\n",
    "after = True\n",
    "#Initializing posts_climate list and params dictionary:\n",
    "posts_climate = []\n",
    "params = {}\n",
    "#Limiting the loop so 2500 or less posts collected: \n",
    "while len(posts_climate) < 2500:\n",
    "    #Making sure that the loop breaks once the final post is collected (when after field is empty):\n",
    "    if after == None:\n",
    "        break\n",
    "    #Collecting posts that will be placed in the posts_climate list:    \n",
    "    else:\n",
    "        #Filling the current post's after value in the params dictionary:\n",
    "        params = {'after': after}\n",
    "    #Requesting data from the Reddit API at https://www.reddit.com/r/climate/new.json\n",
    "    #Using the params dictionary to ensure collecting posts sequentially \n",
    "    #Using headers so instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\":\n",
    "    res = requests.get('https://www.reddit.com/r/climate/new.json', params, headers=headers)\n",
    "    #Using the data in the json format:\n",
    "    the_json = res.json()\n",
    "    #Getting each full post (with all information related to each post):\n",
    "    posts_climate.extend(the_json['data'].get('children'))\n",
    "    #Getting the after value for this post so it can be used in params to collect the next post\n",
    "    after = the_json['data']['after']\n",
    "    #Time delay so appear as a human clicking through the subreddit instead of a computer\n",
    "    #(also trying to respect Reddit - don't want to overload their servers)\n",
    "    time.sleep(1)\n",
    "    #Printing the length of posts_climate to see running total length of \n",
    "    #posts_climate while this cell runs\n",
    "    print(len(posts_climate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "125\n",
      "150\n",
      "175\n",
      "200\n",
      "225\n",
      "250\n",
      "275\n",
      "300\n",
      "325\n",
      "350\n",
      "375\n",
      "400\n",
      "425\n",
      "450\n",
      "475\n",
      "500\n",
      "525\n",
      "550\n",
      "575\n",
      "600\n",
      "625\n",
      "650\n",
      "675\n",
      "700\n",
      "725\n",
      "750\n",
      "775\n",
      "800\n",
      "825\n",
      "850\n",
      "875\n",
      "900\n",
      "925\n",
      "950\n",
      "975\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "#Collecting posts on the r/climateskeptics/new page\n",
    "\n",
    "#Creating a header which will prevent request-related error: \n",
    "#(instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.4\")\n",
    "headers = {'User-agent': 'Julia bot 0.4'}\n",
    "#Initializing after variable as True intil it will be replaced with after values in posts:\n",
    "after = True\n",
    "#Initializing posts_skeptics list and params dictionary:\n",
    "posts_skeptics = []\n",
    "params = {}\n",
    "#Limiting the loop so 2500 or less posts collected:\n",
    "while len(posts_skeptics) < 2500:\n",
    "    #Making sure that the loop breaks once the final post is collected (when after field is empty):\n",
    "    if after == None:\n",
    "        break\n",
    "    #Collecting posts that will be placed in the posts_skeptics list:\n",
    "    else:\n",
    "        #Filling the current post's after value in the params dictionary:\n",
    "        params = {'after': after}\n",
    "    #Requesting data from the Reddit API at https://www.reddit.com/r/climateskeptics/new.json\n",
    "    #Using the params dictionary to ensure collecting posts sequentially \n",
    "    #Using headers so instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.4\":\n",
    "    res = requests.get('https://www.reddit.com/r/climateskeptics/new.json', params, headers=headers)\n",
    "    #Using the data in the json format:\n",
    "    the_json = res.json()\n",
    "    #Getting each full post (with all information related to each post):\n",
    "    posts_skeptics.extend(the_json['data'].get('children'))\n",
    "    #Getting the after value for this post so it can be used in params to collect the next post\n",
    "    after = the_json['data']['after']\n",
    "    #Time delay so appear as a human clicking through the subreddit instead of a computer\n",
    "    #(also trying to respect Reddit - don't want to overload their servers)\n",
    "    time.sleep(1)\n",
    "    #Printing the length of posts_skeptics to see running total length of \n",
    "    #posts_skeptics while this cell runs\n",
    "    print(len(posts_skeptics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting full posts in posts_climate_list and \n",
    "#post titles in post_titles_climate_list\n",
    "\n",
    "posts_climate_list = []\n",
    "post_titles_climate_list = []\n",
    "#Collecting posts and post titles in posts_climate and placing in lists:\n",
    "for i in range(len(posts_climate)):\n",
    "    posts_climate_list.append(posts_climate[i])\n",
    "    post_titles_climate_list.append(posts_climate[i]['data']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>full_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Underwater' homeowners group promotes climate...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Science is not a partisan issue....</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Melting glaciers reveal Everest bodies</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Climate change could push tropical diseases to...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which Will Melt First—GOP's Climate Denial, or...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  'Underwater' homeowners group promotes climate...   \n",
       "1                Science is not a partisan issue....   \n",
       "2             Melting glaciers reveal Everest bodies   \n",
       "3  Climate change could push tropical diseases to...   \n",
       "4  Which Will Melt First—GOP's Climate Denial, or...   \n",
       "\n",
       "                                           full_post  \n",
       "0  {'kind': 't3', 'data': {'approved_at_utc': Non...  \n",
       "1  {'kind': 't3', 'data': {'approved_at_utc': Non...  \n",
       "2  {'kind': 't3', 'data': {'approved_at_utc': Non...  \n",
       "3  {'kind': 't3', 'data': {'approved_at_utc': Non...  \n",
       "4  {'kind': 't3', 'data': {'approved_at_utc': Non...  "
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiating dataframe to place posts and post titles into:\n",
    "df_posts_climate = pd.DataFrame()\n",
    "#Adding post_titles_climate_list to 'title' column of df:\n",
    "df_posts_climate['title'] = post_titles_climate_list\n",
    "#Adding posts_climate_list to 'full_post' column of df:\n",
    "df_posts_climate['full_post'] = posts_climate_list\n",
    "#Inspecting the head of the df:\n",
    "df_posts_climate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(976, 2)"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspecting dimensions of the df:\n",
    "df_posts_climate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting full posts in posts_skeptics_list and \n",
    "#post titles in post_titles_skeptics_list\n",
    "\n",
    "posts_skeptics_list = []\n",
    "post_titles_skeptics_list = []\n",
    "#Collecting posts and post titles in posts_skeptics and placing in lists:\n",
    "for i in range(len(posts_skeptics)):\n",
    "    posts_skeptics_list.append(posts_skeptics[i])\n",
    "    post_titles_skeptics_list.append(posts_skeptics[i]['data']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>full_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In which cases is the pro-AGW alarmist narrati...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senator for Utah - Mike Lee's hilarious commen...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Climate science needs a critical review by unb...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Global Warming Has Been Canceled: Key Greenlan...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Deal of New Green</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  In which cases is the pro-AGW alarmist narrati...   \n",
       "1  Senator for Utah - Mike Lee's hilarious commen...   \n",
       "2  Climate science needs a critical review by unb...   \n",
       "3  Global Warming Has Been Canceled: Key Greenlan...   \n",
       "4                                A Deal of New Green   \n",
       "\n",
       "                                           full_post  \n",
       "0  {'kind': 't3', 'data': {'approved_at_utc': Non...  \n",
       "1  {'kind': 't3', 'data': {'approved_at_utc': Non...  \n",
       "2  {'kind': 't3', 'data': {'approved_at_utc': Non...  \n",
       "3  {'kind': 't3', 'data': {'approved_at_utc': Non...  \n",
       "4  {'kind': 't3', 'data': {'approved_at_utc': Non...  "
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiating dataframe to place posts and post titles into:\n",
    "df_posts_skeptics = pd.DataFrame()\n",
    "#Adding post_titles_skeptics_list to 'title' column of df:\n",
    "df_posts_skeptics['title'] = post_titles_skeptics_list\n",
    "#Adding posts_skeptics_list to 'full_post' column of df:\n",
    "df_posts_skeptics['full_post'] = posts_skeptics_list\n",
    "#Inspecting the head of the df:\n",
    "df_posts_skeptics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 2)"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspecting dimensions of the df:\n",
    "df_posts_skeptics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving dataframe of r/climate/new posts and post titles to a csv in the data file in this repo:\n",
    "df_posts_climate.to_csv('../data/climate_posts_with_titles_20190329_0545.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving dataframe of r/climateskeptics/new posts and post titles to a csv in the data file in this repo:\n",
    "df_posts_skeptics.to_csv('../data/skeptics_posts_with_titles_20190329_0545.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next try to see if can gather comments again-had an error on evening of 3/38/2019 and unable\n",
    "#to collect comments so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing tried to collect r/climate posts and comments, but ran into issues, so commented this out:\n",
    "\n",
    "# headers = {'User-agent': 'Julia bot 0.4'}\n",
    "# climate_comments_all = []\n",
    "\n",
    "# for i in range(len(post_titles_climate_list)):\n",
    "#     res_comments = requests.get('https://www.reddit.com' + \n",
    "#                                 posts_climate[i]['data']['permalink'] +'.json', headers=headers)\n",
    "#     the_json = res_comments.json()\n",
    "#     comments_each_post = []\n",
    "#     for j in range(len(the_json[1]['data']['children'])):        \n",
    "#         try: \n",
    "#             comments_each_post.append(the_json[1]['data']['children'][j]['data']['body'])\n",
    "#         except j == []:\n",
    "#             comments_each_post.append(['No comments'])            \n",
    "        \n",
    "#         time.sleep(1)\n",
    "    \n",
    "#     climate_comments_all.append(comments_each_post)\n",
    "        \n",
    "#     clear_output()\n",
    "#     print(len(climate_comments_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attaining Post Comment json Data to Unpack Later On \n",
    "### (If I decide to use the comments, I will have the data, and I can attain individual comment text and other information from this data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating header and res for a test run (commented a lot of it because it may be case sensitive - \n",
    "#it could cause errors if rerun on a different day when there are no comments on a post, for example)\n",
    "#headers = {'User-agent': 'Julia bot 0.3'}\n",
    "#res = requests.get('https://www.reddit.com/r/climate/new.json', headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "#Making sure no client error when requesting data:\n",
    "#print(res.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using data with json format:\n",
    "#the_json = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['modhash', 'dist', 'children', 'after', 'before'])"
      ]
     },
     "execution_count": 781,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspecting data keys:\n",
    "#the_json['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great Lakes are rapidly warming, likely to trigger more flooding and extreme weather'"
      ]
     },
     "execution_count": 780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspecting a title:\n",
    "#the_json['data']['children'][7]['data']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/r/climate/comments/b7qdyz/i_was_wrong_on_climate_change_why_cant_other/'"
      ]
     },
     "execution_count": 757,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspecting a permalink:\n",
    "#the_json['data']['children'][7]['data']['permalink']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspecting selftext:\n",
    "#the_json['data']['children'][7]['data']['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting comment data:\n",
    "# res_comments = requests.get('https://www.reddit.com'+\n",
    "#                               the_json['data']['children'][7]['data']['permalink'] + '.json',\n",
    "#                               headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "#Making sure no client error when requesting data: \n",
    "#print(res_comments.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using data with json format:\n",
    "#the_json_1 = res_comments.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Inspecting insides of comments data:\n",
    "\n",
    "#the_json_1[1]['data'].get('children')\n",
    "#1st level comments:\n",
    "#the_json_1[1]['data']['children'][0]['data']['body']\n",
    "#Next one:\n",
    "#the_json_1[1]['data']['children'][1]['data']['body']\n",
    "#len(the_json_1[1]['data']['children'])\n",
    "#for j in range(len(the_json_1[1]['data']['children'])):\n",
    "#    print(the_json_1[1]['data']['children'][j]['data']['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Climate DENIERS are likely to be RACIST. Why? https://www.sierraclub.org/sierra/climate-deniers-are-more-likely-be-racist-obama-trump-climate-change'"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2nd level comments\n",
    "#the_json_1[1]['data']['children'][0]['data']['replies']['data']['children'][0]['data']['body']\n",
    "#Next one:\n",
    "#the_json_1[1]['data']['children'][0]['data']['replies']['data']['children'][0]['data']['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'But things are changing. If you look back at the amount of past events occurring such as droughts and heatwaves the frequency is increasing, and though the amount of hurricanes is decreasing their intensity is increasing. Flooding was the most common type of hazard last year and caused huge amounts of death and damages in India, Japan, Bangladesh, these being linked to global sea level rise. So things ARE changing, even if it will take a while before everyone is affected by it, so we should do something.'"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3rd level comments\n",
    "#the_json_1[1]['data']['children'][1]['data']['replies']['data']['children'][0]['data']['replies']['data']['children'][0]['data']['body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection of Data that Was Used for Modeling in Next Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting Data on 01 Apr 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "125\n",
      "150\n",
      "175\n",
      "200\n",
      "225\n",
      "250\n",
      "275\n",
      "300\n",
      "325\n",
      "350\n",
      "375\n",
      "400\n",
      "425\n",
      "450\n",
      "475\n",
      "500\n",
      "525\n",
      "550\n",
      "575\n",
      "600\n",
      "625\n",
      "650\n",
      "675\n",
      "700\n",
      "725\n",
      "750\n",
      "775\n",
      "800\n",
      "825\n",
      "850\n",
      "875\n",
      "900\n",
      "925\n",
      "950\n",
      "975\n",
      "977\n"
     ]
    }
   ],
   "source": [
    "#Collecting subreddit r/climate/new posts\n",
    "\n",
    "#Creating a header which will prevent request-related error: \n",
    "#(instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\")\n",
    "headers = {'User-agent': 'Julia bot 0.3'}\n",
    "#Initializing after variable as True intil it will be replaced with after values in posts:\n",
    "after = True\n",
    "#Initializing posts_climate list and params dictionary:\n",
    "posts_climate = []\n",
    "params = {}\n",
    "#Limiting the loop so 2500 or less climate posts collected in posts_climate:\n",
    "while len(posts_climate) < 2500:\n",
    "    #Making sure that the loop breaks once the final post is collected (when after field is empty):\n",
    "    if after == None:\n",
    "        break\n",
    "    #Collecting posts that will be placed in the posts_climate list:\n",
    "    else:\n",
    "        #Filling the current post's after value in the params dictionary:\n",
    "        params = {'after': after}\n",
    "    #Requesting data from the Reddit API at https://www.reddit.com/r/climate/new.json\n",
    "    #Using the params dictionary to ensure collecting posts sequentially \n",
    "    #Using headers so instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\":\n",
    "    res = requests.get('https://www.reddit.com/r/climate/new.json', params, headers=headers)\n",
    "    #Using the data in the json format:\n",
    "    the_json = res.json()\n",
    "    #Getting each full post (with all information related to each post):\n",
    "    posts_climate.extend(the_json['data'].get('children'))\n",
    "    #Getting the after value for this post so it can be used in params to collect the next post\n",
    "    after = the_json['data']['after']\n",
    "    #Time delay so appear as a human clicking through the subreddit instead of a computer\n",
    "    #(also trying to respect Reddit - don't want to overload their servers)\n",
    "    time.sleep(1)\n",
    "    #Printing the length of posts_climate to see running total length of posts_climate while this cell runs\n",
    "    print(len(posts_climate))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "0\n",
      "13\n",
      "2\n",
      "2\n",
      "0\n",
      "9\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "8\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "5\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "4\n",
      "1\n",
      "2\n",
      "0\n",
      "5\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "4\n",
      "2\n",
      "3\n",
      "5\n",
      "6\n",
      "1\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "4\n",
      "0\n",
      "6\n",
      "3\n",
      "0\n",
      "7\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "5\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "1\n",
      "5\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "6\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "9\n",
      "1\n",
      "0\n",
      "4\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "4\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "8\n",
      "1\n",
      "7\n",
      "4\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "1\n",
      "4\n",
      "8\n",
      "5\n",
      "0\n",
      "1\n",
      "1\n",
      "11\n",
      "9\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "4\n",
      "0\n",
      "3\n",
      "1\n",
      "5\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "5\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "8\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "4\n",
      "6\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "4\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "6\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "7\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "7\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "2\n",
      "1\n",
      "6\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "1\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "4\n",
      "1\n",
      "6\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "5\n",
      "7\n",
      "4\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "7\n",
      "13\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "7\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "4\n",
      "2\n",
      "4\n",
      "8\n",
      "0\n",
      "1\n",
      "0\n",
      "5\n",
      "2\n",
      "3\n",
      "0\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "13\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "4\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "7\n",
      "4\n",
      "2\n",
      "0\n",
      "6\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "11\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "11\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "5\n",
      "4\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "13\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "10\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "7\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "6\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "4\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "4\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "4\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "7\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "5\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "7\n",
      "11\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "4\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "9\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "5\n",
      "0\n",
      "0\n",
      "7\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "6\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "4\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "5\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "6\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "4\n",
      "1\n",
      "2\n",
      "5\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "5\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "4\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "4\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Creating lists of post titles, post permalinks, post text (if any text beneath the title), and post comments\n",
    "#which will be used to build a dataframe of r/climate post information\n",
    "\n",
    "#Instantiating the lists:\n",
    "titles_climate = []\n",
    "permalinks_climate = []\n",
    "post_text_climate = []\n",
    "posts_comments_climate = []\n",
    "#Filling the lists with information from each post in posts_climate:\n",
    "for i in range(len(posts_climate)):\n",
    "    #Getting the title of post i:\n",
    "    titles_climate.append(posts_climate[i]['data']['title'])\n",
    "    #Getting the permalink of post i:\n",
    "    permalinks_climate.append(posts_climate[i]['data']['permalink'])\n",
    "    #Getting the text of post i:\n",
    "    post_text_climate.append(posts_climate[i]['data']['selftext'])\n",
    "        \n",
    "    #Finding the comments for post i:\n",
    "    #Creating a header which will prevent request-related error: \n",
    "    #(instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\")\n",
    "    headers = {'User-agent': 'Julia bot 0.3'}\n",
    "    #Getting the after value for this post so it can be used in params to collect the next post\n",
    "    after = True\n",
    "    #Initializing list post_i_comments_climate and params dictionary: \n",
    "    post_i_comments_climate = []\n",
    "    params = {}\n",
    "    #Making sure that the loop breaks once the final post i comment is collected (when after field is empty):\n",
    "    if after == None:\n",
    "        break\n",
    "    #Collecting post i comments that will be placed in the post_i_comments_climate list:\n",
    "    else:\n",
    "        #Filling the current comment's after value in the params dictionary:\n",
    "        params = {'after': after}\n",
    "    #Requesting data from the Reddit API at https://www.reddit.com/r/climate/new/[post permalink].json\n",
    "    #Using the params dictionary to ensure collecting post comments sequentially \n",
    "    #Using headers so instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\":\n",
    "    res_comments = requests.get('https://www.reddit.com'+\n",
    "                                posts_climate[i]['data']['permalink'] + '.json',\n",
    "                                params,\n",
    "                                headers=headers)\n",
    "    #Using the data (comments for post i) in the json format:\n",
    "    the_json_1 = res_comments.json()\n",
    "    #Getting each post's full comments page (with all information related to comments for each post):\n",
    "    post_i_comments_climate.extend(the_json_1[1]['data'].get('children'))\n",
    "    #Getting the after value for this comment so it can be used in params to collect the next comment:\n",
    "    after = the_json_1[1]['data']['after']\n",
    "    #Time delay so appear as a human clicking through the subreddit instead of a computer\n",
    "    #(also trying to respect Reddit - don't want to overload their servers)\n",
    "    time.sleep(1)\n",
    "    #Printing the number of top-level comments (other comments embedded) for each post\n",
    "    print(len(post_i_comments_climate))\n",
    "    #Appending post_i_comments_climate list to posts_comments_climate list of comment lists:\n",
    "    posts_comments_climate.append(post_i_comments_climate)\n",
    "#Another time delay before collecing next post's comments\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "977\n"
     ]
    }
   ],
   "source": [
    "#Checking length of titles_climate list (should be same as length of posts_climate)\n",
    "print(len(titles_climate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "977\n"
     ]
    }
   ],
   "source": [
    "#Checking length of posts_climate\n",
    "print(len(posts_climate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "977\n"
     ]
    }
   ],
   "source": [
    "#Checking length of permalinks_climate list (should be same as length of posts_climate)\n",
    "print(len(permalinks_climate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "977\n"
     ]
    }
   ],
   "source": [
    "#Checking length of post_text_climate (should be same as length of posts_climate)\n",
    "print(len(post_text_climate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10747\n"
     ]
    }
   ],
   "source": [
    "#Checking length of posts_comments_climate\n",
    "print(len(posts_comments_climate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>full_post</th>\n",
       "      <th>permalink</th>\n",
       "      <th>post_text</th>\n",
       "      <th>full_comments_page_each_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is anomalous? The stabilisation of CH₄ co...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climate/comments/b8364j/what_is_anomalous_t...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Global carbon emissions hit record high in 201...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climate/comments/b82tsw/global_carbon_emiss...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.S. judge scraps Trump order opening Arctic, ...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climate/comments/b82cw2/us_judge_scraps_tru...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ocasio-Cortez slams critics who would wait on ...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climate/comments/b826lv/ocasiocortez_slams_...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Save the climate</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climate/comments/b8214y/save_the_climate/</td>\n",
       "      <td>Ok, we need to save the climate. Start to post...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  What is anomalous? The stabilisation of CH₄ co...   \n",
       "1  Global carbon emissions hit record high in 201...   \n",
       "2  U.S. judge scraps Trump order opening Arctic, ...   \n",
       "3  Ocasio-Cortez slams critics who would wait on ...   \n",
       "4                                   Save the climate   \n",
       "\n",
       "                                           full_post  \\\n",
       "0  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "1  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "2  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "3  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "4  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  /r/climate/comments/b8364j/what_is_anomalous_t...   \n",
       "1  /r/climate/comments/b82tsw/global_carbon_emiss...   \n",
       "2  /r/climate/comments/b82cw2/us_judge_scraps_tru...   \n",
       "3  /r/climate/comments/b826lv/ocasiocortez_slams_...   \n",
       "4       /r/climate/comments/b8214y/save_the_climate/   \n",
       "\n",
       "                                           post_text  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                                      \n",
       "4  Ok, we need to save the climate. Start to post...   \n",
       "\n",
       "  full_comments_page_each_post  \n",
       "0                           []  \n",
       "1                           []  \n",
       "2                           []  \n",
       "3                           []  \n",
       "4                           []  "
      ]
     },
     "execution_count": 846,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiating dataframe to place posts, post titles, post permalinks, and post comments into:\n",
    "df_posts_climate = pd.DataFrame()\n",
    "#Adding post_titles_climate_list to 'title' column of df:\n",
    "df_posts_climate['title'] = titles_climate\n",
    "#Adding posts_climate to 'full_post' column of df:\n",
    "df_posts_climate['full_post'] = posts_climate\n",
    "#Adding permalinks_climate to 'permalink' column of df:\n",
    "df_posts_climate['permalink'] = permalinks_climate\n",
    "#Adding post_text_climate to 'post_text' column of df:\n",
    "df_posts_climate['post_text'] = post_text_climate\n",
    "#Adding posts_comments_climate to 'full_comments_page_each_post' column of df:\n",
    "df_posts_climate['full_comments_page_each_post'] = posts_comments_climate\n",
    "#Inspecting the head of the df:\n",
    "df_posts_climate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(977, 5)"
      ]
     },
     "execution_count": 847,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking dimensions of df_posts_climate (should be length of posts_climate, 5):\n",
    "df_posts_climate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving dataframe of r/climate/new posts, post titles, post permalinks, and post comments to a csv \n",
    "#in the data file in this repo:\n",
    "df_posts_climate.to_csv('../data/climate_posts_detailed_20190401_0955.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#Making sure will be able to unpack comments later on:\n",
    "\n",
    "#Getting level 1 comments for each post:\n",
    "#post_i_comments_level_1 = []\n",
    "#for j in range(len(the_json_1[1]['data']['children'])):\n",
    "#    try:\n",
    "#        post_i_comments_level_1.append(the_json_1[1]['data']['children'][j]['data']['body'])\n",
    "#    except:\n",
    "#        post_i_comments_level_1.append(\"None\")\n",
    "#Adding each post's list of 1st level comments to post_comments_level_1\n",
    "#post_comments_level_1.extend(post_i_comments_level_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "125\n",
      "150\n",
      "175\n",
      "200\n",
      "225\n",
      "250\n",
      "275\n",
      "300\n",
      "325\n",
      "350\n",
      "375\n",
      "400\n",
      "425\n",
      "450\n",
      "475\n",
      "500\n",
      "525\n",
      "550\n",
      "575\n",
      "600\n",
      "625\n",
      "650\n",
      "675\n",
      "700\n",
      "725\n",
      "750\n",
      "775\n",
      "800\n",
      "825\n",
      "850\n",
      "875\n",
      "900\n",
      "925\n",
      "950\n",
      "975\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "#Collecting posts on the r/climateskeptics/new page\n",
    "\n",
    "#Creating a header which will prevent request-related error: \n",
    "#(instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\")\n",
    "headers = {'User-agent': 'Julia bot 0.3'}\n",
    "#Initializing after variable as True intil it will be replaced with after values in posts:\n",
    "after = True\n",
    "#Initializing posts_skeptics list and params dictionary:\n",
    "posts_skeptics = []\n",
    "params = {}\n",
    "#Limiting the loop so 2500 or less posts collected:\n",
    "while len(posts_skeptics) < 2500:\n",
    "    #Making sure that the loop breaks once the final post is collected (when after field is empty):\n",
    "    if after == None:\n",
    "        break\n",
    "    #Collecting posts that will be placed in the posts_skeptics list:\n",
    "    else:\n",
    "        #Filling the current post's after value in the params dictionary:\n",
    "        params = {'after': after}\n",
    "    #Requesting data from the Reddit API at https://www.reddit.com/r/climateskeptics/new.json\n",
    "    #Using the params dictionary to ensure collecting posts sequentially \n",
    "    #Using headers so instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\":\n",
    "    res = requests.get('https://www.reddit.com/r/climateskeptics/new.json', params, headers=headers)\n",
    "    #Using the data in the json format:\n",
    "    the_json = res.json()\n",
    "    #Getting each full post (with all information related to each post):\n",
    "    posts_skeptics.extend(the_json['data'].get('children'))\n",
    "    #Getting the after value for this post so it can be used in params to collect the next post\n",
    "    after = the_json['data']['after']\n",
    "    #Time delay so appear as a human clicking through the subreddit instead of a computer\n",
    "    #(also trying to respect Reddit - don't want to overload their servers)\n",
    "    time.sleep(1)\n",
    "    #Printing the length of posts_skeptics to see running total length of \n",
    "    #posts_skeptics while this cell runs\n",
    "    print(len(posts_skeptics))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "4\n",
      "0\n",
      "0\n",
      "7\n",
      "6\n",
      "0\n",
      "2\n",
      "0\n",
      "4\n",
      "8\n",
      "2\n",
      "5\n",
      "3\n",
      "7\n",
      "1\n",
      "1\n",
      "6\n",
      "0\n",
      "6\n",
      "6\n",
      "2\n",
      "5\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "4\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "6\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "5\n",
      "10\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "7\n",
      "8\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "5\n",
      "2\n",
      "0\n",
      "1\n",
      "5\n",
      "2\n",
      "0\n",
      "5\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "1\n",
      "0\n",
      "4\n",
      "4\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "4\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "8\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "14\n",
      "1\n",
      "3\n",
      "0\n",
      "10\n",
      "0\n",
      "0\n",
      "17\n",
      "2\n",
      "1\n",
      "1\n",
      "9\n",
      "0\n",
      "4\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "4\n",
      "6\n",
      "2\n",
      "6\n",
      "4\n",
      "2\n",
      "7\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "14\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "10\n",
      "9\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "2\n",
      "4\n",
      "2\n",
      "0\n",
      "16\n",
      "4\n",
      "1\n",
      "5\n",
      "6\n",
      "0\n",
      "4\n",
      "1\n",
      "10\n",
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "6\n",
      "2\n",
      "9\n",
      "2\n",
      "1\n",
      "0\n",
      "4\n",
      "1\n",
      "6\n",
      "0\n",
      "7\n",
      "0\n",
      "0\n",
      "0\n",
      "17\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "4\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "7\n",
      "2\n",
      "0\n",
      "9\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "8\n",
      "0\n",
      "3\n",
      "5\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "7\n",
      "9\n",
      "5\n",
      "9\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "4\n",
      "0\n",
      "3\n",
      "5\n",
      "4\n",
      "7\n",
      "0\n",
      "2\n",
      "2\n",
      "6\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "4\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "5\n",
      "9\n",
      "7\n",
      "3\n",
      "1\n",
      "1\n",
      "7\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      "3\n",
      "8\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "4\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "7\n",
      "3\n",
      "0\n",
      "7\n",
      "2\n",
      "1\n",
      "6\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "8\n",
      "2\n",
      "6\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "9\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "1\n",
      "5\n",
      "1\n",
      "3\n",
      "1\n",
      "0\n",
      "10\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "8\n",
      "1\n",
      "2\n",
      "4\n",
      "1\n",
      "5\n",
      "4\n",
      "4\n",
      "1\n",
      "8\n",
      "0\n",
      "6\n",
      "2\n",
      "5\n",
      "1\n",
      "5\n",
      "2\n",
      "0\n",
      "0\n",
      "5\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "1\n",
      "6\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "5\n",
      "1\n",
      "4\n",
      "0\n",
      "33\n",
      "5\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "3\n",
      "4\n",
      "3\n",
      "2\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "4\n",
      "1\n",
      "4\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "14\n",
      "0\n",
      "18\n",
      "1\n",
      "3\n",
      "1\n",
      "4\n",
      "3\n",
      "0\n",
      "1\n",
      "9\n",
      "4\n",
      "3\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "3\n",
      "2\n",
      "9\n",
      "1\n",
      "3\n",
      "3\n",
      "8\n",
      "7\n",
      "1\n",
      "5\n",
      "1\n",
      "0\n",
      "5\n",
      "2\n",
      "3\n",
      "6\n",
      "0\n",
      "7\n",
      "2\n",
      "8\n",
      "1\n",
      "10\n",
      "1\n",
      "4\n",
      "0\n",
      "2\n",
      "2\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "18\n",
      "6\n",
      "2\n",
      "0\n",
      "11\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "4\n",
      "6\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "4\n",
      "2\n",
      "0\n",
      "3\n",
      "9\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "4\n",
      "0\n",
      "5\n",
      "3\n",
      "1\n",
      "8\n",
      "4\n",
      "6\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "4\n",
      "1\n",
      "17\n",
      "1\n",
      "0\n",
      "1\n",
      "3\n",
      "7\n",
      "3\n",
      "1\n",
      "7\n",
      "3\n",
      "3\n",
      "8\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "6\n",
      "1\n",
      "11\n",
      "3\n",
      "7\n",
      "2\n",
      "4\n",
      "4\n",
      "0\n",
      "2\n",
      "1\n",
      "4\n",
      "2\n",
      "7\n",
      "5\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "7\n",
      "3\n",
      "0\n",
      "6\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "8\n",
      "1\n",
      "0\n",
      "5\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "8\n",
      "3\n",
      "3\n",
      "5\n",
      "1\n",
      "5\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "4\n",
      "3\n",
      "1\n",
      "5\n",
      "9\n",
      "3\n",
      "1\n",
      "1\n",
      "10\n",
      "3\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "7\n",
      "4\n",
      "3\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "6\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "4\n",
      "2\n",
      "3\n",
      "3\n",
      "0\n",
      "4\n",
      "1\n",
      "3\n",
      "2\n",
      "6\n",
      "2\n",
      "6\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "4\n",
      "3\n",
      "2\n",
      "2\n",
      "1\n",
      "7\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "6\n",
      "3\n",
      "1\n",
      "3\n",
      "0\n",
      "8\n",
      "0\n",
      "4\n",
      "2\n",
      "3\n",
      "8\n",
      "1\n",
      "1\n",
      "3\n",
      "7\n",
      "4\n",
      "1\n",
      "12\n",
      "2\n",
      "12\n",
      "1\n",
      "6\n",
      "2\n",
      "2\n",
      "5\n",
      "0\n",
      "2\n",
      "16\n",
      "9\n",
      "0\n",
      "1\n",
      "1\n",
      "4\n",
      "4\n",
      "1\n",
      "2\n",
      "4\n",
      "7\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "5\n",
      "0\n",
      "2\n",
      "4\n",
      "4\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "8\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "4\n",
      "3\n",
      "4\n",
      "2\n",
      "1\n",
      "1\n",
      "5\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "0\n",
      "6\n",
      "12\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "1\n",
      "8\n",
      "4\n",
      "2\n",
      "1\n",
      "7\n",
      "4\n",
      "0\n",
      "7\n",
      "1\n",
      "1\n",
      "12\n",
      "0\n",
      "4\n",
      "2\n",
      "7\n",
      "5\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "5\n",
      "14\n",
      "3\n",
      "2\n",
      "3\n",
      "5\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      "4\n",
      "6\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "10\n",
      "2\n",
      "5\n",
      "2\n",
      "5\n",
      "12\n",
      "10\n",
      "0\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "12\n",
      "2\n",
      "4\n",
      "2\n",
      "6\n",
      "2\n",
      "3\n",
      "1\n",
      "4\n",
      "4\n",
      "7\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "6\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "5\n",
      "3\n",
      "11\n",
      "3\n",
      "3\n",
      "0\n",
      "1\n",
      "4\n",
      "20\n",
      "7\n",
      "5\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "0\n",
      "1\n",
      "6\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "5\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "4\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "4\n",
      "0\n",
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "5\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "5\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "4\n",
      "9\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "4\n",
      "3\n",
      "4\n",
      "2\n",
      "3\n",
      "4\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "12\n",
      "1\n",
      "5\n",
      "4\n",
      "6\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "15\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "7\n",
      "3\n",
      "3\n",
      "1\n",
      "4\n",
      "2\n",
      "1\n",
      "2\n",
      "5\n",
      "4\n",
      "1\n",
      "3\n",
      "0\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "7\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "8\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n",
      "8\n",
      "11\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "5\n",
      "3\n",
      "1\n",
      "2\n",
      "4\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Creating lists of post titles, post permalinks, post text (if any text beneath the title), and post comments\n",
    "#which will be used to build a dataframe of r/climateskeptics post information\n",
    "\n",
    "#Instantiating the lists:\n",
    "titles_skeptics = []\n",
    "permalinks_skeptics = []\n",
    "post_text_skeptics = []\n",
    "posts_comments_skeptics = []\n",
    "#Filling the lists with information from each post in posts_skeptics:\n",
    "for i in range(len(posts_skeptics)):\n",
    "    #Getting the title of post i:\n",
    "    titles_skeptics.append(posts_skeptics[i]['data']['title'])\n",
    "    #Getting the permalink of post i:\n",
    "    permalinks_skeptics.append(posts_skeptics[i]['data']['permalink'])\n",
    "    #Getting the text of post i:\n",
    "    post_text_skeptics.append(posts_skeptics[i]['data']['selftext'])\n",
    "        \n",
    "    #Finding the comments for post i:\n",
    "    #Creating a header which will prevent request-related error: \n",
    "    #(instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\")\n",
    "    headers = {'User-agent': 'Julia bot 0.3'}\n",
    "    #Getting the after value for this post so it can be used in params to collect the next post\n",
    "    after = True\n",
    "    #Initializing list post_i_comments_skeptics and params dictionary: \n",
    "    post_i_comments_skeptics = []\n",
    "    params = {}\n",
    "    #Making sure that the loop breaks once the final post i comment is collected (when after field is empty):\n",
    "    if after == None:\n",
    "        break\n",
    "    #Collecting post i comments that will be placed in the post_i_comments_skeptics list:\n",
    "    else:\n",
    "        #Filling the current comment's after value in the params dictionary:\n",
    "        params = {'after': after}\n",
    "    #Requesting data from the Reddit API at https://www.reddit.com/r/climateskeptics/new/[post permalink].json\n",
    "    #Using the params dictionary to ensure collecting post comments sequentially \n",
    "    #Using headers so instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\":\n",
    "    res_comments = requests.get('https://www.reddit.com'+\n",
    "                                posts_skeptics[i]['data']['permalink'] + '.json',\n",
    "                                params,\n",
    "                                headers=headers)\n",
    "    #Using the data (comments for post i) in the json format:\n",
    "    the_json_1 = res_comments.json()\n",
    "    #Getting each post's full comments page (with all information related to comments for each post):\n",
    "    post_i_comments_skeptics.extend(the_json_1[1]['data'].get('children'))\n",
    "    #Getting the after value for this comment so it can be used in params to collect the next comment:\n",
    "    after = the_json_1[1]['data']['after']\n",
    "    #Time delay so appear as a human clicking through the subreddit instead of a computer\n",
    "    #(also trying to respect Reddit - don't want to overload their servers)\n",
    "    time.sleep(1)\n",
    "    #Printing the number of top-level comments (other comments embedded) for each post\n",
    "    print(len(post_i_comments_skeptics)) \n",
    "    #Appending post_i_comments_skeptics list to posts_comments_skeptics list of comment lists:\n",
    "    posts_comments_skeptics.append(post_i_comments_skeptics)\n",
    "#Another time delay before collecing next post's comments\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking length of posts_comments_skeptics same as length of posts_skeptics:\n",
    "len(posts_comments_skeptics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>full_post</th>\n",
       "      <th>permalink</th>\n",
       "      <th>post_text</th>\n",
       "      <th>full_comments_page_each_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Climate Change Alarmists Routinely Ignore Chin...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climateskeptics/comments/b82s1h/climate_cha...</td>\n",
       "      <td></td>\n",
       "      <td>[{'kind': 't1', 'data': {'subreddit_id': 't5_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Conversation: Lets Create a Youth Climate ...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climateskeptics/comments/b7zono/the_convers...</td>\n",
       "      <td></td>\n",
       "      <td>[{'kind': 't1', 'data': {'subreddit_id': 't5_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NASA Now Altering ‘Unadjusted” Data To Create ...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climateskeptics/comments/b7x0qk/nasa_now_al...</td>\n",
       "      <td></td>\n",
       "      <td>[{'kind': 't1', 'data': {'subreddit_id': 't5_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Australia: Labor tries to have it both ways on...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climateskeptics/comments/b7w3h7/australia_l...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I made a book list. Feel free to contribute.</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climateskeptics/comments/b7v3qo/i_made_a_bo...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Climate Change Alarmists Routinely Ignore Chin...   \n",
       "1  The Conversation: Lets Create a Youth Climate ...   \n",
       "2  NASA Now Altering ‘Unadjusted” Data To Create ...   \n",
       "3  Australia: Labor tries to have it both ways on...   \n",
       "4       I made a book list. Feel free to contribute.   \n",
       "\n",
       "                                           full_post  \\\n",
       "0  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "1  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "2  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "3  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "4  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "\n",
       "                                           permalink post_text  \\\n",
       "0  /r/climateskeptics/comments/b82s1h/climate_cha...             \n",
       "1  /r/climateskeptics/comments/b7zono/the_convers...             \n",
       "2  /r/climateskeptics/comments/b7x0qk/nasa_now_al...             \n",
       "3  /r/climateskeptics/comments/b7w3h7/australia_l...             \n",
       "4  /r/climateskeptics/comments/b7v3qo/i_made_a_bo...             \n",
       "\n",
       "                        full_comments_page_each_post  \n",
       "0  [{'kind': 't1', 'data': {'subreddit_id': 't5_2...  \n",
       "1  [{'kind': 't1', 'data': {'subreddit_id': 't5_2...  \n",
       "2  [{'kind': 't1', 'data': {'subreddit_id': 't5_2...  \n",
       "3                                                 []  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiating dataframe to place posts, post titles, post permalinks, and post comments into:\n",
    "df_posts_skeptics = pd.DataFrame()\n",
    "#Adding titles_skeptics to 'title' column of df:\n",
    "df_posts_skeptics['title'] = titles_skeptics\n",
    "#Adding posts_skeptics to 'full_post' column of df:\n",
    "df_posts_skeptics['full_post'] = posts_skeptics\n",
    "#Adding permalinks_skeptics to 'permalink' column of df:\n",
    "df_posts_skeptics['permalink'] = permalinks_skeptics\n",
    "#Adding post_text_skeptics to 'post_text' column of df:\n",
    "df_posts_skeptics['post_text'] = post_text_skeptics\n",
    "#Adding posts_comments_skeptics to 'full_comments_page_each_post' column of df:\n",
    "df_posts_skeptics['full_comments_page_each_post'] = posts_comments_skeptics\n",
    "#Inspecting the head of the df:\n",
    "df_posts_skeptics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving dataframe of r/climateskeptics/new posts, post titles, post permalinks, and post comments to a csv \n",
    "#in the data file in this repo:\n",
    "df_posts_skeptics.to_csv('../data/skeptics_posts_detailed_20190401_1040.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting Data on 02 Apr 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries just in case:\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "125\n",
      "150\n",
      "175\n",
      "200\n",
      "225\n",
      "250\n",
      "275\n",
      "300\n",
      "325\n",
      "350\n",
      "375\n",
      "400\n",
      "425\n",
      "450\n",
      "475\n",
      "500\n",
      "525\n",
      "550\n",
      "575\n",
      "600\n",
      "625\n",
      "650\n",
      "675\n",
      "700\n",
      "725\n",
      "750\n",
      "775\n",
      "800\n",
      "825\n",
      "850\n",
      "875\n",
      "900\n",
      "925\n",
      "950\n",
      "975\n",
      "976\n"
     ]
    }
   ],
   "source": [
    "#Collecting subreddit r/climate/new posts\n",
    "\n",
    "#Creating a header which will prevent request-related error: \n",
    "#(instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\")\n",
    "headers = {'User-agent': 'Julia bot 0.3'}\n",
    "#Initializing after variable as True intil it will be replaced with after values in posts:\n",
    "after = True\n",
    "#Initializing posts_climate list and params dictionary:\n",
    "posts_climate = []\n",
    "params = {}\n",
    "\n",
    "#Limiting the loop so 2500 or less climate posts collected in posts_climate: \n",
    "while len(posts_climate) < 2500:\n",
    "    #Making sure that the loop breaks once the final post is collected (when after field is empty):\n",
    "    if after == None:\n",
    "        break\n",
    "    #Collecting posts that will be placed in the posts_climate list:\n",
    "    else:\n",
    "        #Filling the current post's after value in the params dictionary:\n",
    "        params = {'after': after}\n",
    "    #Requesting data from the Reddit API at https://www.reddit.com/r/climate/new.json\n",
    "    #Using the params dictionary to ensure collecting posts sequentially \n",
    "    #Using headers so instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\":\n",
    "    res = requests.get('https://www.reddit.com/r/climate/new.json', params, headers=headers)\n",
    "    #Using the data in the json format:\n",
    "    the_json = res.json()\n",
    "    #Getting each full post (with all information related to each post):\n",
    "    posts_climate.extend(the_json['data'].get('children'))\n",
    "    #Getting the after value for this post so it can be used in params to collect the next post\n",
    "    after = the_json['data']['after']\n",
    "    #Time delay so appear as a human clicking through the subreddit instead of a computer\n",
    "    #(also trying to respect Reddit - don't want to overload their servers)\n",
    "    time.sleep(1)\n",
    "    #Printing the length of posts_climate to see running total length of posts_climate while this cell runs\n",
    "    print(len(posts_climate))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "7\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "0\n",
      "13\n",
      "2\n",
      "2\n",
      "0\n",
      "9\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "8\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "5\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "4\n",
      "1\n",
      "2\n",
      "0\n",
      "5\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "4\n",
      "2\n",
      "3\n",
      "5\n",
      "6\n",
      "1\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "4\n",
      "0\n",
      "6\n",
      "3\n",
      "0\n",
      "7\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "5\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "1\n",
      "5\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "6\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "9\n",
      "1\n",
      "0\n",
      "4\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "4\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "8\n",
      "1\n",
      "7\n",
      "4\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "1\n",
      "4\n",
      "8\n",
      "5\n",
      "0\n",
      "1\n",
      "1\n",
      "11\n",
      "9\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "4\n",
      "0\n",
      "3\n",
      "1\n",
      "5\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "5\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "8\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "4\n",
      "6\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "4\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "6\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "7\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "7\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "2\n",
      "1\n",
      "6\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "1\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "4\n",
      "1\n",
      "6\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "5\n",
      "7\n",
      "4\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "7\n",
      "13\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "7\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "4\n",
      "2\n",
      "4\n",
      "8\n",
      "0\n",
      "1\n",
      "0\n",
      "5\n",
      "2\n",
      "3\n",
      "0\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "13\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "4\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "7\n",
      "4\n",
      "2\n",
      "0\n",
      "6\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "11\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "11\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "5\n",
      "4\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "13\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "10\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "7\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "6\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "4\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "4\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "4\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "7\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "5\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "7\n",
      "11\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "4\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "9\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "5\n",
      "0\n",
      "0\n",
      "7\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "6\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "4\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "5\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "6\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "4\n",
      "1\n",
      "2\n",
      "5\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "5\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "4\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "4\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Creating lists of post titles, post permalinks, post text (if any text beneath the title), and post comments\n",
    "#which will be used to build a dataframe of r/climate post information\n",
    "\n",
    "#Instantiating the lists:\n",
    "titles_climate = []\n",
    "permalinks_climate = []\n",
    "post_text_climate = []\n",
    "posts_comments_climate = []\n",
    "#Filling the lists with information from each post in posts_climate:\n",
    "for i in range(len(posts_climate)):\n",
    "    #Getting the title of post i:\n",
    "    titles_climate.append(posts_climate[i]['data']['title'])\n",
    "    #Getting the permalink of post i:\n",
    "    permalinks_climate.append(posts_climate[i]['data']['permalink'])\n",
    "    #Getting the text of post i:\n",
    "    post_text_climate.append(posts_climate[i]['data']['selftext'])\n",
    "        \n",
    "    #Finding the comments for post i:\n",
    "    #Creating a header which will prevent request-related error: \n",
    "    #(instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\")\n",
    "    headers = {'User-agent': 'Julia bot 0.3'}\n",
    "    #Getting the after value for this post so it can be used in params to collect the next post\n",
    "    after = True\n",
    "    #Initializing list post_i_comments_climate and params dictionary: \n",
    "    post_i_comments_climate = []\n",
    "    params = {}\n",
    "    #Making sure that the loop breaks once the final post i comment is collected (when after field is empty):\n",
    "    if after == None:\n",
    "        break\n",
    "    #Collecting post i comments that will be placed in the post_i_comments_climate list:\n",
    "    else:\n",
    "        #Filling the current comment's after value in the params dictionary:\n",
    "        params = {'after': after}\n",
    "    #Requesting data from the Reddit API at https://www.reddit.com/r/climate/new/[post permalink].json\n",
    "    #Using the params dictionary to ensure collecting post comments sequentially \n",
    "    #Using headers so instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\":\n",
    "    res_comments = requests.get('https://www.reddit.com'+\n",
    "                                posts_climate[i]['data']['permalink'] + '.json',\n",
    "                                params,\n",
    "                                headers=headers)\n",
    "    #Using the data (comments for post i) in the json format:\n",
    "    the_json_1 = res_comments.json()\n",
    "    #Getting each post's full comments page (with all information related to comments for each post):\n",
    "    post_i_comments_climate.extend(the_json_1[1]['data'].get('children'))\n",
    "    #Getting the after value for this comment so it can be used in params to collect the next comment:\n",
    "    after = the_json_1[1]['data']['after']\n",
    "    #Time delay so appear as a human clicking through the subreddit instead of a computer\n",
    "    #(also trying to respect Reddit - don't want to overload their servers)\n",
    "    time.sleep(1)\n",
    "    #Printing the number of top-level comments (other comments embedded) for each post:\n",
    "    print(len(post_i_comments_climate)) \n",
    "    #Appending post_i_comments_climate list to posts_comments_climate list of comment lists:\n",
    "    posts_comments_climate.append(post_i_comments_climate)\n",
    "#Another time delay before collecing next post's comments\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>full_post</th>\n",
       "      <th>permalink</th>\n",
       "      <th>post_text</th>\n",
       "      <th>full_comments_page_each_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resources to objectively debate with people wh...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climate/comments/b8r4rw/resources_to_object...</td>\n",
       "      <td>What resources can I read/view to easily under...</td>\n",
       "      <td>[{'kind': 't1', 'data': {'subreddit_id': 't5_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A World Without Water. A short film inspired b...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climate/comments/b8qb80/a_world_without_wat...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Polar Warning: Even Antarctica’s Coldest Regio...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climate/comments/b8pl16/polar_warning_even_...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chemical distributor makes tasteless April Foo...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climate/comments/b8o1ug/chemical_distributo...</td>\n",
       "      <td></td>\n",
       "      <td>[{'kind': 't1', 'data': {'subreddit_id': 't5_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shell quits major US oil lobby over climate ch...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climate/comments/b8nt6l/shell_quits_major_u...</td>\n",
       "      <td></td>\n",
       "      <td>[{'kind': 't1', 'data': {'subreddit_id': 't5_2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Resources to objectively debate with people wh...   \n",
       "1  A World Without Water. A short film inspired b...   \n",
       "2  Polar Warning: Even Antarctica’s Coldest Regio...   \n",
       "3  Chemical distributor makes tasteless April Foo...   \n",
       "4  Shell quits major US oil lobby over climate ch...   \n",
       "\n",
       "                                           full_post  \\\n",
       "0  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "1  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "2  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "3  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "4  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  /r/climate/comments/b8r4rw/resources_to_object...   \n",
       "1  /r/climate/comments/b8qb80/a_world_without_wat...   \n",
       "2  /r/climate/comments/b8pl16/polar_warning_even_...   \n",
       "3  /r/climate/comments/b8o1ug/chemical_distributo...   \n",
       "4  /r/climate/comments/b8nt6l/shell_quits_major_u...   \n",
       "\n",
       "                                           post_text  \\\n",
       "0  What resources can I read/view to easily under...   \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "                        full_comments_page_each_post  \n",
       "0  [{'kind': 't1', 'data': {'subreddit_id': 't5_2...  \n",
       "1                                                 []  \n",
       "2                                                 []  \n",
       "3  [{'kind': 't1', 'data': {'subreddit_id': 't5_2...  \n",
       "4  [{'kind': 't1', 'data': {'subreddit_id': 't5_2...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiating dataframe to place posts, post titles, post permalinks, and post comments into:\n",
    "df_posts_climate = pd.DataFrame()\n",
    "#Adding titles_climate to 'title' column of df:\n",
    "df_posts_climate['title'] = titles_climate\n",
    "#Adding posts_climate to 'full_post' column of df:\n",
    "df_posts_climate['full_post'] = posts_climate\n",
    "#Adding permalinks_climate to 'permalink' column of df:\n",
    "df_posts_climate['permalink'] = permalinks_climate\n",
    "#Adding post_text_climate to 'post_text' column of df:\n",
    "df_posts_climate['post_text'] = post_text_climate\n",
    "#Adding posts_comments_climate to 'full_comments_page_each_post' column of df:\n",
    "df_posts_climate['full_comments_page_each_post'] = posts_comments_climate\n",
    "#Inspecting the head of the df:\n",
    "df_posts_climate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving dataframe of r/climate/new posts, post titles, post permalinks, and post comments to a csv \n",
    "#in the data file in this repo:\n",
    "df_posts_climate.to_csv('../data/climate_posts_detailed_20190402_2100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "125\n",
      "150\n",
      "175\n",
      "200\n",
      "225\n",
      "250\n",
      "275\n",
      "300\n",
      "325\n",
      "350\n",
      "375\n",
      "400\n",
      "425\n",
      "450\n",
      "475\n",
      "500\n",
      "525\n",
      "550\n",
      "575\n",
      "600\n",
      "625\n",
      "650\n",
      "675\n",
      "700\n",
      "725\n",
      "750\n",
      "775\n",
      "800\n",
      "825\n",
      "850\n",
      "875\n",
      "900\n",
      "925\n",
      "950\n",
      "975\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "#Collecting subreddit r/climateskeptics/new posts\n",
    "\n",
    "#Creating a header which will prevent request-related error: \n",
    "#(instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\")\n",
    "headers = {'User-agent': 'Julia bot 0.3'}\n",
    "#Initializing after variable as True intil it will be replaced with after values in posts:\n",
    "after = True\n",
    "#Initializing posts_skeptics list and params dictionary:\n",
    "posts_skeptics = []\n",
    "params = {}\n",
    "\n",
    "#Limiting the loop so 2500 or less climateskeptics posts collected in posts_skeptics: \n",
    "while len(posts_skeptics) < 2500:\n",
    "    #Making sure that the loop breaks once the final post is collected (when after field is empty):\n",
    "    if after == None:\n",
    "        break\n",
    "    #Collecting posts that will be placed in the posts_skeptics list:\n",
    "    else:\n",
    "        #Filling the current post's after value in the params dictionary:\n",
    "        params = {'after': after}\n",
    "    #Requesting data from the Reddit API at https://www.reddit.com/r/climateskeptics/new.json\n",
    "    #Using the params dictionary to ensure collecting posts sequentially \n",
    "    #Using headers so instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\":\n",
    "    res = requests.get('https://www.reddit.com/r/climateskeptics/new.json', params, headers=headers)\n",
    "    #Using the data in the json format:\n",
    "    the_json = res.json()\n",
    "    #Getting each full post (with all information related to each post):\n",
    "    posts_skeptics.extend(the_json['data'].get('children'))\n",
    "    #Getting the after value for this post so it can be used in params to collect the next post\n",
    "    after = the_json['data']['after']\n",
    "    #Time delay so appear as a human clicking through the subreddit instead of a computer\n",
    "    #(also trying to respect Reddit - don't want to overload their servers)\n",
    "    time.sleep(1)\n",
    "    #Printing the length of posts_skeptics to see running total length of posts_skeptics while this cell runs\n",
    "    print(len(posts_skeptics))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "3\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "4\n",
      "4\n",
      "7\n",
      "0\n",
      "0\n",
      "7\n",
      "6\n",
      "0\n",
      "2\n",
      "0\n",
      "4\n",
      "8\n",
      "2\n",
      "5\n",
      "3\n",
      "7\n",
      "1\n",
      "1\n",
      "6\n",
      "0\n",
      "6\n",
      "6\n",
      "2\n",
      "5\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "6\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "5\n",
      "10\n",
      "0\n",
      "0\n",
      "2\n",
      "5\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "7\n",
      "8\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "5\n",
      "2\n",
      "0\n",
      "1\n",
      "5\n",
      "2\n",
      "0\n",
      "5\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "1\n",
      "0\n",
      "4\n",
      "4\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "4\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "8\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "14\n",
      "1\n",
      "3\n",
      "0\n",
      "10\n",
      "0\n",
      "0\n",
      "17\n",
      "2\n",
      "1\n",
      "1\n",
      "9\n",
      "0\n",
      "4\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "4\n",
      "6\n",
      "2\n",
      "6\n",
      "4\n",
      "2\n",
      "7\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "14\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "10\n",
      "9\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "2\n",
      "4\n",
      "2\n",
      "0\n",
      "16\n",
      "4\n",
      "1\n",
      "5\n",
      "6\n",
      "0\n",
      "4\n",
      "1\n",
      "10\n",
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "6\n",
      "2\n",
      "9\n",
      "2\n",
      "1\n",
      "0\n",
      "4\n",
      "1\n",
      "6\n",
      "0\n",
      "7\n",
      "0\n",
      "0\n",
      "0\n",
      "17\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "4\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "7\n",
      "2\n",
      "0\n",
      "9\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "8\n",
      "0\n",
      "3\n",
      "5\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "7\n",
      "9\n",
      "5\n",
      "9\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "4\n",
      "0\n",
      "3\n",
      "5\n",
      "4\n",
      "7\n",
      "0\n",
      "2\n",
      "2\n",
      "6\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "4\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "5\n",
      "9\n",
      "7\n",
      "3\n",
      "1\n",
      "1\n",
      "7\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      "3\n",
      "8\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "4\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "7\n",
      "3\n",
      "0\n",
      "7\n",
      "2\n",
      "1\n",
      "6\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "8\n",
      "2\n",
      "6\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "9\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "1\n",
      "5\n",
      "1\n",
      "3\n",
      "1\n",
      "0\n",
      "10\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "8\n",
      "1\n",
      "2\n",
      "4\n",
      "1\n",
      "5\n",
      "4\n",
      "4\n",
      "1\n",
      "8\n",
      "0\n",
      "6\n",
      "2\n",
      "5\n",
      "1\n",
      "5\n",
      "2\n",
      "0\n",
      "0\n",
      "5\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "1\n",
      "6\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "5\n",
      "1\n",
      "4\n",
      "0\n",
      "33\n",
      "5\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "3\n",
      "4\n",
      "3\n",
      "2\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "4\n",
      "1\n",
      "4\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "14\n",
      "0\n",
      "18\n",
      "1\n",
      "3\n",
      "1\n",
      "4\n",
      "3\n",
      "0\n",
      "1\n",
      "9\n",
      "4\n",
      "3\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "3\n",
      "2\n",
      "9\n",
      "1\n",
      "3\n",
      "3\n",
      "8\n",
      "7\n",
      "1\n",
      "5\n",
      "1\n",
      "0\n",
      "5\n",
      "2\n",
      "3\n",
      "6\n",
      "0\n",
      "7\n",
      "2\n",
      "8\n",
      "1\n",
      "10\n",
      "1\n",
      "4\n",
      "0\n",
      "2\n",
      "2\n",
      "4\n",
      "0\n",
      "1\n",
      "0\n",
      "18\n",
      "6\n",
      "2\n",
      "0\n",
      "11\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "4\n",
      "6\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "4\n",
      "2\n",
      "0\n",
      "3\n",
      "9\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "4\n",
      "0\n",
      "5\n",
      "3\n",
      "1\n",
      "8\n",
      "4\n",
      "6\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "4\n",
      "1\n",
      "17\n",
      "1\n",
      "0\n",
      "1\n",
      "3\n",
      "7\n",
      "3\n",
      "1\n",
      "7\n",
      "3\n",
      "3\n",
      "8\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "6\n",
      "1\n",
      "11\n",
      "3\n",
      "7\n",
      "2\n",
      "4\n",
      "4\n",
      "0\n",
      "2\n",
      "1\n",
      "4\n",
      "2\n",
      "7\n",
      "5\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "7\n",
      "3\n",
      "0\n",
      "6\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "8\n",
      "1\n",
      "0\n",
      "5\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "8\n",
      "3\n",
      "3\n",
      "5\n",
      "1\n",
      "5\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "4\n",
      "3\n",
      "1\n",
      "5\n",
      "9\n",
      "3\n",
      "1\n",
      "1\n",
      "10\n",
      "3\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "7\n",
      "4\n",
      "3\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "6\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "4\n",
      "2\n",
      "3\n",
      "3\n",
      "0\n",
      "4\n",
      "1\n",
      "3\n",
      "2\n",
      "6\n",
      "2\n",
      "6\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "4\n",
      "3\n",
      "2\n",
      "2\n",
      "1\n",
      "7\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "6\n",
      "3\n",
      "1\n",
      "3\n",
      "0\n",
      "8\n",
      "0\n",
      "4\n",
      "2\n",
      "3\n",
      "8\n",
      "1\n",
      "1\n",
      "3\n",
      "7\n",
      "4\n",
      "1\n",
      "12\n",
      "2\n",
      "12\n",
      "1\n",
      "6\n",
      "2\n",
      "2\n",
      "5\n",
      "0\n",
      "2\n",
      "16\n",
      "9\n",
      "0\n",
      "1\n",
      "1\n",
      "4\n",
      "4\n",
      "1\n",
      "2\n",
      "4\n",
      "7\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "5\n",
      "0\n",
      "2\n",
      "4\n",
      "4\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "8\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "4\n",
      "3\n",
      "4\n",
      "2\n",
      "1\n",
      "1\n",
      "5\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "0\n",
      "6\n",
      "12\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "1\n",
      "8\n",
      "4\n",
      "2\n",
      "1\n",
      "7\n",
      "4\n",
      "0\n",
      "7\n",
      "1\n",
      "1\n",
      "12\n",
      "0\n",
      "4\n",
      "2\n",
      "7\n",
      "5\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "5\n",
      "14\n",
      "3\n",
      "2\n",
      "3\n",
      "5\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      "4\n",
      "6\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "10\n",
      "2\n",
      "5\n",
      "2\n",
      "5\n",
      "12\n",
      "10\n",
      "0\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "12\n",
      "2\n",
      "4\n",
      "2\n",
      "6\n",
      "2\n",
      "3\n",
      "1\n",
      "4\n",
      "4\n",
      "7\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "6\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "5\n",
      "3\n",
      "11\n",
      "3\n",
      "3\n",
      "0\n",
      "1\n",
      "4\n",
      "20\n",
      "7\n",
      "5\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "0\n",
      "1\n",
      "6\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "5\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "4\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "4\n",
      "0\n",
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "5\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "5\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "4\n",
      "9\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "4\n",
      "3\n",
      "4\n",
      "2\n",
      "3\n",
      "4\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "12\n",
      "1\n",
      "5\n",
      "4\n",
      "6\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "15\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "7\n",
      "3\n",
      "3\n",
      "1\n",
      "4\n",
      "2\n",
      "1\n",
      "2\n",
      "5\n",
      "4\n",
      "1\n",
      "3\n",
      "0\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "7\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "8\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n",
      "8\n",
      "11\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Creating lists of post titles, post permalinks, post text (if any text beneath the title), and post comments\n",
    "#which will be used to build a dataframe of r/climateskeptics post information\n",
    "\n",
    "#Instantiating the lists:\n",
    "titles_skeptics = []\n",
    "permalinks_skeptics = []\n",
    "post_text_skeptics = []\n",
    "posts_comments_skeptics = []\n",
    "#Filling the lists with information from each post in posts_skeptics:\n",
    "for i in range(len(posts_skeptics)):\n",
    "    #Getting the title of post i:\n",
    "    titles_skeptics.append(posts_skeptics[i]['data']['title'])\n",
    "    #Getting the permalink of post i:\n",
    "    permalinks_skeptics.append(posts_skeptics[i]['data']['permalink'])\n",
    "    #Getting the text of post i:\n",
    "    post_text_skeptics.append(posts_skeptics[i]['data']['selftext'])\n",
    "        \n",
    "    #Finding the comments for post i:\n",
    "    #Creating a header which will prevent request-related error: \n",
    "    #(instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\")\n",
    "    headers = {'User-agent': 'Julia bot 0.3'}\n",
    "    #Getting the after value for this post so it can be used in params to collect the next post\n",
    "    after = True\n",
    "    #Initializing list post_i_comments_skeptics and params dictionary: \n",
    "    post_i_comments_skeptics = []\n",
    "    params = {}\n",
    "    #Making sure that the loop breaks once the final post i comment is collected (when after field is empty):\n",
    "    if after == None:\n",
    "        break\n",
    "    #Collecting post i comments that will be placed in the post_i_comments_skeptics list:\n",
    "    else:\n",
    "        #Filling the current comment's after value in the params dictionary:\n",
    "        params = {'after': after}\n",
    "    #Requesting data from the Reddit API at https://www.reddit.com/r/climateskeptics/new/[post permalink].json\n",
    "    #Using the params dictionary to ensure collecting post comments sequentially \n",
    "    #Using headers so instead of showing up as a computer program looking for data, appearing as \"Julia bot 0.3\":\n",
    "    res_comments = requests.get('https://www.reddit.com'+\n",
    "                                posts_skeptics[i]['data']['permalink'] + '.json',\n",
    "                                params,\n",
    "                                headers=headers)\n",
    "    #Using the data (comments for post i) in the json format:\n",
    "    the_json_1 = res_comments.json()\n",
    "    #Getting each post's full comments page (with all information related to comments for each post):\n",
    "    post_i_comments_skeptics.extend(the_json_1[1]['data'].get('children'))\n",
    "    #Getting the after value for this comment so it can be used in params to collect the next comment:\n",
    "    after = the_json_1[1]['data']['after']\n",
    "    #Time delay so appear as a human clicking through the subreddit instead of a computer\n",
    "    #(also trying to respect Reddit - don't want to overload their servers)\n",
    "    time.sleep(1)\n",
    "    #Printing the number of top-level comments (other comments embedded) for each post\n",
    "    print(len(post_i_comments_skeptics)) \n",
    "    #Appending post_i_comments_skeptics list to posts_comments_skeptics list of comment lists:\n",
    "    posts_comments_skeptics.append(post_i_comments_skeptics)\n",
    "#Another time delay before collecing next post's comments\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking length of posts_comments_skeptics same as length of posts_skeptics:\n",
    "len(posts_comments_skeptics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>full_post</th>\n",
       "      <th>permalink</th>\n",
       "      <th>post_text</th>\n",
       "      <th>full_comments_page_each_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TIL that despite an above average sea level ri...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climateskeptics/comments/b8w68y/til_that_de...</td>\n",
       "      <td></td>\n",
       "      <td>[{'kind': 't1', 'data': {'subreddit_id': 't5_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What the alarmists won't tell you about Arctic...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climateskeptics/comments/b8tyid/what_the_al...</td>\n",
       "      <td>So the url below, from government sources, cle...</td>\n",
       "      <td>[{'kind': 't1', 'data': {'subreddit_id': 't5_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Lost History of One of the World’s Strange...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climateskeptics/comments/b8tsml/the_lost_hi...</td>\n",
       "      <td></td>\n",
       "      <td>[{'kind': 't1', 'data': {'subreddit_id': 't5_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pelosi’s New Climate Bill Binds America to Par...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climateskeptics/comments/b6pxcp/pelosis_new...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Global lightning activity and the hiatus in gl...</td>\n",
       "      <td>{'kind': 't3', 'data': {'approved_at_utc': Non...</td>\n",
       "      <td>/r/climateskeptics/comments/b8qo7d/global_ligh...</td>\n",
       "      <td></td>\n",
       "      <td>[{'kind': 't1', 'data': {'subreddit_id': 't5_2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  TIL that despite an above average sea level ri...   \n",
       "1  What the alarmists won't tell you about Arctic...   \n",
       "2  The Lost History of One of the World’s Strange...   \n",
       "3  Pelosi’s New Climate Bill Binds America to Par...   \n",
       "4  Global lightning activity and the hiatus in gl...   \n",
       "\n",
       "                                           full_post  \\\n",
       "0  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "1  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "2  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "3  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "4  {'kind': 't3', 'data': {'approved_at_utc': Non...   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  /r/climateskeptics/comments/b8w68y/til_that_de...   \n",
       "1  /r/climateskeptics/comments/b8tyid/what_the_al...   \n",
       "2  /r/climateskeptics/comments/b8tsml/the_lost_hi...   \n",
       "3  /r/climateskeptics/comments/b6pxcp/pelosis_new...   \n",
       "4  /r/climateskeptics/comments/b8qo7d/global_ligh...   \n",
       "\n",
       "                                           post_text  \\\n",
       "0                                                      \n",
       "1  So the url below, from government sources, cle...   \n",
       "2                                                      \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "                        full_comments_page_each_post  \n",
       "0  [{'kind': 't1', 'data': {'subreddit_id': 't5_2...  \n",
       "1  [{'kind': 't1', 'data': {'subreddit_id': 't5_2...  \n",
       "2  [{'kind': 't1', 'data': {'subreddit_id': 't5_2...  \n",
       "3                                                 []  \n",
       "4  [{'kind': 't1', 'data': {'subreddit_id': 't5_2...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiating dataframe to place posts, post titles, post permalinks, and post comments into:\n",
    "df_posts_skeptics = pd.DataFrame()\n",
    "#Adding titles_skeptics to 'title' column of df:\n",
    "df_posts_skeptics['title'] = titles_skeptics\n",
    "#Adding posts_skeptics to 'full_post' column of df:\n",
    "df_posts_skeptics['full_post'] = posts_skeptics\n",
    "#Adding permalinks_skeptics to 'permalink' column of df:\n",
    "df_posts_skeptics['permalink'] = permalinks_skeptics\n",
    "#Adding post_text_skeptics to 'post_text' column of df:\n",
    "df_posts_skeptics['post_text'] = post_text_skeptics\n",
    "#Adding posts_comments_skeptics to 'full_comments_page_each_post' column of df:\n",
    "df_posts_skeptics['full_comments_page_each_post'] = posts_comments_skeptics\n",
    "#Inspecting the head of the df:\n",
    "df_posts_skeptics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving dataframe of r/climateskeptics/new posts, post titles, post permalinks, and post comments to a csv \n",
    "#in the data file in this repo:\n",
    "df_posts_skeptics.to_csv('../data/skeptics_posts_detailed_20190403_0640.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to collect more data, but I ran into time constraints.  In the future, I would like to collect more data to build a better model.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
